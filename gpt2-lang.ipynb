{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2150,"sourceType":"datasetVersion","datasetId":29}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:21:58.731093Z","iopub.execute_input":"2025-03-22T06:21:58.731398Z","iopub.status.idle":"2025-03-22T06:21:58.735335Z","shell.execute_reply.started":"2025-03-22T06:21:58.731368Z","shell.execute_reply":"2025-03-22T06:21:58.734691Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load and prepare initial data \ndataset_path = \"/kaggle/input/climate-change-earth-surface-temperature-data/GlobalLandTemperaturesByCity.csv\"\ndf = pd.read_csv(dataset_path).dropna()\ndf = df[['dt', 'AverageTemperature', 'City', 'Country']]\ndf['dt'] = pd.to_datetime(df['dt'])\ncountries = ['Germany', 'France', 'United Kingdom', 'United States', 'Canada', \n             'Australia', 'India', 'China', 'Japan', 'Brazil']\ndf = df[df['Country'].isin(countries)]\ndf = df[(df['dt'].dt.year >= 1900) & (df['dt'].dt.year <= 2000)]\n\ndef calculate_trend(group):\n    group = group.sort_values('dt')\n    start_year = group['dt'].min().year\n    end_year = group['dt'].max().year\n    if len(group) >= 2:\n        start_temp = group.iloc[0]['AverageTemperature']\n        end_temp = group.iloc[-1]['AverageTemperature']\n        trend = end_temp - start_temp\n        return pd.Series({'start_year': start_year, 'end_year': end_year, 'trend': trend})\n    return pd.Series({'start_year': None, 'end_year': None, 'trend': None})\n\ntrend_df = df.groupby(['City', 'Country']).apply(calculate_trend, include_groups=False).reset_index()\ntrend_df = trend_df.dropna()\n\ninstructions = []\nresponses = []\nfor _, row in trend_df.iterrows():\n    instruction = f\"What is the temperature trend in {row['City']}, {row['Country']} from {int(row['start_year'])} to {int(row['end_year'])}?\"\n    response = f\"The temperature trend in {row['City']}, {row['Country']} from {int(row['start_year'])} to {int(row['end_year'])} was {row['trend']:.2f}°C\"\n    instructions.append(instruction)\n    responses.append(response)\n\ninstruction_response_df = pd.DataFrame({'instruction': instructions, 'response': responses})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:22:03.428545Z","iopub.execute_input":"2025-03-22T06:22:03.428823Z","iopub.status.idle":"2025-03-22T06:22:22.664983Z","shell.execute_reply.started":"2025-03-22T06:22:03.428799Z","shell.execute_reply":"2025-03-22T06:22:22.664192Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\n# 1. Prepare training data\ndef prepare_training_data(instruction_response_df):\n    expanded_data = []\n    for _, row in instruction_response_df.iterrows():\n        city_country_match = re.search(r'in ([^,]+), ([^,]+) from (\\d+) to (\\d+)', row['instruction'])\n        if city_country_match:\n            city = city_country_match.group(1)\n            country = city_country_match.group(2)\n            start_year = city_country_match.group(3)\n            end_year = city_country_match.group(4)\n            temp_match = re.search(r'was ([-+]?\\d+\\.\\d+)°C', row['response'])\n            if temp_match:\n                temp_trend = temp_match.group(1)\n                variations = [\n                    {\"input\": f\"What is the temperature trend in {city}, {country} from {start_year} to {end_year}?\",\n                     \"output\": f\"The temperature trend in {city}, {country} from {start_year} to {end_year} was {temp_trend}°C.\"},\n                    {\"input\": f\"What's the temperature trend in {city}, {country} between {start_year} and {end_year}?\",\n                     \"output\": f\"Between {start_year} and {end_year}, {city}, {country} experienced a temperature change of {temp_trend}°C.\"},\n                    {\"input\": f\"How did temperatures change in {city}, {country} from {start_year}–{end_year}?\",\n                     \"output\": f\"In {city}, {country}, temperatures changed by {temp_trend}°C from {start_year} to {end_year}.\"},\n                    {\"input\": f\"From {start_year} to {end_year}, what was the temperature trend in {city}, {country}?\",\n                     \"output\": f\"From {start_year} to {end_year}, {city}, {country} saw a temperature trend of {temp_trend}°C.\"}\n                ]\n                expanded_data.extend(variations)\n    \n    return pd.DataFrame(expanded_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:22:34.992546Z","iopub.execute_input":"2025-03-22T06:22:34.992835Z","iopub.status.idle":"2025-03-22T06:22:34.998701Z","shell.execute_reply.started":"2025-03-22T06:22:34.992813Z","shell.execute_reply":"2025-03-22T06:22:34.997861Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# 2. Prepare dataset for training\ndef prepare_dataset(df, tokenizer, max_length=128):\n    inputs = df['input'].tolist()\n    outputs = df['output'].tolist()\n    input_ids_list = []\n    attention_masks_list = []\n    labels_list = []\n    \n    tokenizer.pad_token = tokenizer.eos_token\n    half_length = max_length // 2\n    \n    for input_text, output_text in zip(inputs, outputs):\n        input_text = f\"input: {input_text} output:\"\n        input_encoding = tokenizer(input_text, max_length=half_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n        output_encoding = tokenizer(output_text, max_length=half_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n        \n        input_ids = torch.cat([input_encoding['input_ids'].squeeze(0), output_encoding['input_ids'].squeeze(0)])\n        attention_mask = torch.cat([input_encoding['attention_mask'].squeeze(0), output_encoding['attention_mask'].squeeze(0)])\n        labels = torch.cat([torch.full((half_length,), -100), output_encoding['input_ids'].squeeze(0)])\n        \n        input_ids_list.append(input_ids)\n        attention_masks_list.append(attention_mask)\n        labels_list.append(labels)\n    \n    return {\n        'input_ids': torch.stack(input_ids_list),\n        'attention_mask': torch.stack(attention_masks_list),\n        'labels': torch.stack(labels_list)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:22:38.698946Z","iopub.execute_input":"2025-03-22T06:22:38.699232Z","iopub.status.idle":"2025-03-22T06:22:38.705427Z","shell.execute_reply.started":"2025-03-22T06:22:38.699211Z","shell.execute_reply":"2025-03-22T06:22:38.704466Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\n# 3. Custom training function\ndef train_model(model, tokenizer, dataset, batch_size=4, learning_rate=5e-5, epochs=3):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    train_dataloader = DataLoader(\n        torch.utils.data.TensorDataset(dataset['input_ids'], dataset['attention_mask'], dataset['labels']),\n        batch_size=batch_size,\n        shuffle=True\n    )\n    \n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n            \n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n        \n        avg_loss = total_loss / len(train_dataloader)\n        print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n    \n    model.save_pretrained(\"/kaggle/working/gpt2_finetuned_custom\")\n    tokenizer.save_pretrained(\"/kaggle/working/gpt2_finetuned_custom\")\n    return \"/kaggle/working/gpt2_finetuned_custom\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:22:42.570372Z","iopub.execute_input":"2025-03-22T06:22:42.570934Z","iopub.status.idle":"2025-03-22T06:22:42.576855Z","shell.execute_reply.started":"2025-03-22T06:22:42.570904Z","shell.execute_reply":"2025-03-22T06:22:42.576054Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 4. Finetuning function\ndef finetune_gpt2_custom(instruction_response_df):\n    train_df = prepare_training_data(instruction_response_df)\n    \n    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n    special_tokens = {\"additional_special_tokens\": [\"<|endoftext|>\", \"input:\", \"output:\"]}\n    tokenizer.add_special_tokens(special_tokens)\n    model.resize_token_embeddings(len(tokenizer))\n    \n    dataset = prepare_dataset(train_df, tokenizer, max_length=128)\n    model_path = train_model(model, tokenizer, dataset, batch_size=4, learning_rate=5e-5, epochs=3)\n    \n    print(f\"Model fine-tuned and saved to {model_path}\")\n    return model_path\n\n# Run finetuning\nmodel_path = finetune_gpt2_custom(instruction_response_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:22:45.725016Z","iopub.execute_input":"2025-03-22T06:22:45.725305Z","iopub.status.idle":"2025-03-22T06:30:51.205355Z","shell.execute_reply.started":"2025-03-22T06:22:45.725282Z","shell.execute_reply":"2025-03-22T06:30:51.204565Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e13bfb6cbf4c4babbfdebcfc2d39481b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d0e51e2ce8c47af821ec627e3591fd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5d059c26abd4d7683bf21972b2be082"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbd13399a13d4d43b2298be68e1580dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f368e87c4f04912bfe1566aadb747d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"548e4f16a16642d8ae112624d30e229e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e01ba55251243ab983dfa7c0a6e0884"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1/3: 100%|██████████| 1629/1629 [02:38<00:00, 10.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Average Loss: 0.1093\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3: 100%|██████████| 1629/1629 [02:37<00:00, 10.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/3, Average Loss: 0.0702\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3: 100%|██████████| 1629/1629 [02:36<00:00, 10.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/3, Average Loss: 0.0463\nModel fine-tuned and saved to /kaggle/working/gpt2_finetuned_custom\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install langchain langchain-community faiss-cpu sentence-transformers transformers\nimport pandas as pd\nimport re\nimport torch\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.docstore.document import Document\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Load the fine-tuned model and tokenizer\nmodel_path = \"/kaggle/working/gpt2_finetuned_custom\"  # Update with your actual path\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Prepare RAG system\ndef setup_rag_system(instruction_response_df):\n    # Create documents for vector store\n    documents = [\n        Document(\n            page_content=row['response'], \n            metadata={\"instruction\": row['instruction']}\n        ) for _, row in instruction_response_df.iterrows()\n    ]\n    \n    # Initialize embeddings\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n    \n    # Create vector store\n    vector_store = FAISS.from_documents(documents, embeddings)\n    \n    return vector_store\n\ndef extract_numerical_data(text):\n    \"\"\"Extract temperature trend value from text\"\"\"\n    temp_match = re.search(r'([-+]?\\d+\\.\\d+)°C', text)\n    if temp_match:\n        return temp_match.group(1)\n    return None\n\ndef generate_response(query, retrieved_doc, model, tokenizer, device):\n    \"\"\"Generate a response using the fine-tuned model\"\"\"\n    # Extract numerical data to ensure accuracy\n    temp_trend = extract_numerical_data(retrieved_doc)\n    \n    # Prepare input for the model\n    input_text = f\"input: {query} output:\"\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n    \n    # Generate response\n    try:\n        with torch.no_grad():\n            output = model.generate(\n                input_ids,\n                max_length=100,\n                temperature=0.7,\n                top_k=50,\n                top_p=0.95,\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id,\n                num_return_sequences=1\n            )\n        \n        # Decode the output\n        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n        \n        # Extract just the output part\n        response = generated_text.replace(input_text, \"\").strip()\n        \n        # Ensure the response contains the correct temperature trend\n        if temp_trend and temp_trend not in response:\n            # Find any temperature values in the response\n            existing_temp = re.search(r'([-+]?\\d+\\.\\d+)°C', response)\n            if existing_temp:\n                # Replace with the correct value\n                response = response.replace(existing_temp.group(1), temp_trend)\n            else:\n                # Add the correct value if not present\n                response = f\"{response} The temperature trend was {temp_trend}°C.\"\n        \n        return response\n    except Exception as e:\n        print(f\"Error generating response: {e}\")\n        return retrieved_doc\n\ndef answer_query(query, vector_store, model, tokenizer, device, k=1, debug=False):\n    \"\"\"Answer a query using the RAG system\"\"\"\n    # Retrieve relevant documents\n    retriever = vector_store.as_retriever(search_kwargs={\"k\": k})\n    retrieved_docs = retriever.invoke(query)\n    \n    if not retrieved_docs:\n        return \"I don't have information about that temperature trend.\"\n    \n    # Get the most relevant document\n    retrieved_text = retrieved_docs[0].page_content\n    \n    if debug:\n        print(f\"Query: {query}\")\n        print(f\"Retrieved: {retrieved_text}\")\n    \n    # Generate enhanced response\n    response = generate_response(query, retrieved_text, model, tokenizer, device)\n    \n    if debug:\n        print(f\"Enhanced: {response}\")\n        \n    return response\n\ndef batch_answer(queries, vector_store, model, tokenizer, device, debug=False):\n    \"\"\"Answer a batch of queries\"\"\"\n    results = []\n    for query in queries:\n        result = answer_query(query, vector_store, model, tokenizer, device, debug=debug)\n        results.append({\"query\": query, \"answer\": result})\n    return results\n\n# Example usage\ndef example_usage(instruction_response_df):\n    # Initialize RAG system\n    vector_store = setup_rag_system(instruction_response_df)\n    \n    # Test queries\n    queries = [\n        \"What is the temperature trend in Berlin, Germany from 1900 to 2000?\",\n        \"What's the temperature trend in Berlin, Germany between 1900 and 2000?\",\n        \"How did temperatures change in Berlin, Germany from 1900–2000?\",\n        \"From 1900 to 2000, what was the temperature trend in Berlin, Germany?\",\n        \"What is the temperature change in Berlin, Germany from 1900 to 2000?\",\n        \"What is the temperature trend in Abiko, Japan from 1900 to 2000?\",\n    ]\n    \n    # Answer queries\n    results = batch_answer(queries, vector_store, model, tokenizer, device, debug=True)\n    \n    # Print results\n    for result in results:\n        print(f\"\\nQuery: {result['query']}\")\n        print(f\"Answer: {result['answer']}\")\n\n# Use the system (would need to call with actual dataframe)\n example_usage(instruction_response_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T06:35:37.491376Z","iopub.execute_input":"2025-03-22T06:35:37.491720Z","iopub.status.idle":"2025-03-22T06:35:49.557501Z","shell.execute_reply.started":"2025-03-22T06:35:37.491692Z","shell.execute_reply":"2025-03-22T06:35:49.556745Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\nCollecting langchain-community\n  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.12)\nCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\nRequirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\nRequirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\nRequirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.11.0a2)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\nCollecting langchain-core<0.4.0,>=0.3.25 (from langchain)\n  Downloading langchain_core-0.3.47-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain\n  Downloading langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nCollecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain)\n  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.29.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.29.0)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.22.4->langchain) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.22.4->langchain) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.22.4->langchain) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.22.4->langchain) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.22.4->langchain) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\nDownloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain-0.3.21-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading faiss_cpu-1.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (30.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain_core-0.3.47-py3-none-any.whl (417 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.1/417.1 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\nDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nInstalling collected packages: python-dotenv, httpx-sse, async-timeout, pydantic-settings, langchain-core, langchain-text-splitters, langchain, langchain-community, faiss-cpu\n  Attempting uninstall: async-timeout\n    Found existing installation: async-timeout 5.0.1\n    Uninstalling async-timeout-5.0.1:\n      Successfully uninstalled async-timeout-5.0.1\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.25\n    Uninstalling langchain-core-0.3.25:\n      Successfully uninstalled langchain-core-0.3.25\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.3\n    Uninstalling langchain-text-splitters-0.3.3:\n      Successfully uninstalled langchain-text-splitters-0.3.3\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.12\n    Uninstalling langchain-0.3.12:\n      Successfully uninstalled langchain-0.3.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed async-timeout-4.0.3 faiss-cpu-1.10.0 httpx-sse-0.4.0 langchain-0.3.21 langchain-community-0.3.20 langchain-core-0.3.47 langchain-text-splitters-0.3.7 pydantic-settings-2.8.1 python-dotenv-1.0.1\n","output_type":"stream"}],"execution_count":8}]}